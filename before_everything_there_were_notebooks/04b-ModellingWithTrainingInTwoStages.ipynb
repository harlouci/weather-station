{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be68fa4a-9012-4826-a75d-0416c1844c7c",
   "metadata": {},
   "source": [
    "# 04b - Modelling with training in two stages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d812e960-3d55-4b1c-a38c-7e189c70cd2b",
   "metadata": {},
   "source": [
    "__Goal__: Same as notebook `04a`, except that first we train and evaluate the models on the `train` and `val` splits, then we retrain and evaluate the models on the `train_val` and `test` splits, where `train_val` is the concatenation of  `train` and `val`.\n",
    "\n",
    "1. Logistic regression\n",
    "2. Linear SVC\n",
    "3. SVC with RBF kernel\n",
    "4. Decision Tree\n",
    "5. Random Forest\n",
    "6. Test metrics consistancy with pikled pipelines and  models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4c9d0f-06da-48cc-8389-a991a7ea4ce2",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5f14228-7a03-478c-955b-c3455727df4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    ConfusionMatrixDisplay,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from weather.transformers.skl_transformer_makers import (\n",
    "    FeatureNames,\n",
    "    TargetChoice,\n",
    "    make_dataset_ingestion_transformer,\n",
    "    make_target_creation_transformer,\n",
    "    make_remove_horizonless_rows_transformer, \n",
    "    make_predictors_feature_engineering_transformer,\n",
    ")\n",
    "from weather.data.prep_datasets import (\n",
    "    prepare_binary_classification_tabular_data, \n",
    "    transform_dataset_and_create_target,\n",
    ")\n",
    "from weather.helpers.utils import camel_to_snake\n",
    "from weather.models.skl_train_models import (\n",
    "    score_evaluation,\n",
    "    train_val_score_evaluation, \n",
    "    train_test_score_evaluation,\n",
    "    print_accuracy_results,\n",
    "    confusion_matrix_evaluation,\n",
    "    confusion_matrix_display,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86e6102-c19a-4031-b008-2a7ec362f711",
   "metadata": {},
   "source": [
    "### Set the directory paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31e47fa1-30d6-411f-8281-fed699d5d1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir =  Path.cwd().parent / \"data\"\n",
    "models_dir = Path.cwd().parent / \"models_trained_in_two_stages\"\n",
    "models_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d73004e-6e11-49e5-aab9-6b917c4c8be2",
   "metadata": {},
   "source": [
    "# 1. Prepare the `dataset` for modelization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74e823b-30ee-453b-8931-cc304e059ae4",
   "metadata": {},
   "source": [
    "### Select the predictors and set the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61482ed5-956f-4b18-9efa-469a2fcd1ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the predictors\n",
    "feature_names = FeatureNames(\n",
    "    numerical=[\n",
    "        \"Temperature\",\n",
    "        \"Humidity\",\n",
    "        \"Wind_speed\",\n",
    "        \"Wind_bearing\",\n",
    "        \"Visibility\",\n",
    "        \"Pressure\",\n",
    "    ],\n",
    "    categorical=[],  # Add or remove \"Weather\", \"Month\" to the predictors\n",
    ")\n",
    "\n",
    "# Set \"Weather\" within 4 hours as target\n",
    "target_name = \"Weather\"\n",
    "horizon = 4\n",
    "target_choice = TargetChoice(target_name, horizon) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14540f18-da50-4d32-b1eb-71515116d2f0",
   "metadata": {},
   "source": [
    "### Set the dataset transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05033867-8cb3-499b-92a3-12d33f0138b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "oldnames_newnames_dict = {\n",
    "    \"Temperature_C\": \"Temperature\", \n",
    "    \"Apparent_Temperature_C\": \"Apparent_temperature\",\n",
    "    \"Wind_speed_kmph\": \"Wind_speed\",\n",
    "    \"Wind_bearing_degrees\": \"Wind_bearing\",\n",
    "    \"Visibility_km\": \"Visibility\",\n",
    "    \"Pressure_millibars\": \"Pressure\",\n",
    "    \"Weather_conditions\": \"Weather\"}\n",
    "\n",
    "dataset_ingestion_transformer = make_dataset_ingestion_transformer(target_choice, oldnames_newnames_dict)\n",
    "remove_horizonless_rows_transformer = make_remove_horizonless_rows_transformer(target_choice)\n",
    "target_creation_transformer = make_target_creation_transformer(target_choice)                       \n",
    "predictors_feature_engineering_transformer = make_predictors_feature_engineering_transformer(feature_names, target_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d6299f-c160-4294-95f1-6d919513e5ea",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e868205-9670-4633-b7b3-60e62821b4ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S_No</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Location</th>\n",
       "      <th>Temperature_C</th>\n",
       "      <th>Apparent_Temperature_C</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind_speed_kmph</th>\n",
       "      <th>Wind_bearing_degrees</th>\n",
       "      <th>Visibility_km</th>\n",
       "      <th>Pressure_millibars</th>\n",
       "      <th>Weather_conditions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2881</td>\n",
       "      <td>2006-01-01 00:00:00+00:00</td>\n",
       "      <td>Port of Turku, Finland</td>\n",
       "      <td>1.161111</td>\n",
       "      <td>-3.238889</td>\n",
       "      <td>0.85</td>\n",
       "      <td>16.6152</td>\n",
       "      <td>139</td>\n",
       "      <td>9.9015</td>\n",
       "      <td>1016.15</td>\n",
       "      <td>rain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   S_No                  Timestamp                Location  Temperature_C  \\\n",
       "0  2881  2006-01-01 00:00:00+00:00  Port of Turku, Finland       1.161111   \n",
       "\n",
       "   Apparent_Temperature_C  Humidity  Wind_speed_kmph  Wind_bearing_degrees  \\\n",
       "0               -3.238889      0.85          16.6152                   139   \n",
       "\n",
       "   Visibility_km  Pressure_millibars Weather_conditions  \n",
       "0         9.9015             1016.15               rain  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(data_dir / 'weather_dataset_raw_development.csv')\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4394929a-f30d-401c-839d-fb4cf9957c82",
   "metadata": {},
   "source": [
    "### Transform the dataset and split  it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55323804-bf99-4d98-9415-f01fd1e9cc60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Three transformers: \"dataset__ingestion_transformer\", \"remove_horizonless_rows_transformer\", \"target_creation_transformer\"\n",
    "transformed_data, created_target = transform_dataset_and_create_target(\n",
    "    df,   \n",
    "    dataset_ingestion_transformer,\n",
    "    remove_horizonless_rows_transformer,\n",
    "    target_creation_transformer,\n",
    ")\n",
    "\n",
    "# Split the dataset\n",
    "dataset = prepare_binary_classification_tabular_data(\n",
    "    transformed_data,\n",
    "    created_target,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb136ae2-e36a-4d33-bbef-549920aab427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30673, 4382, 8765, 30673, 4382, 8765)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.train_x), len(dataset.val_x), len (dataset.test_x), len(dataset.train_y), len(dataset.val_y), len(dataset.test_y)\n",
    "# (30673, 4382, 8765, 30673, 4382, 8765)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18a5b01-0365-4b3c-8433-9bfaa2dbaab2",
   "metadata": {},
   "source": [
    "# 2. Model the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcc1a49-0084-4a9d-9c6b-e6888b63a74e",
   "metadata": {},
   "source": [
    "### Define candidate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca2e57b0-280d-43b1-9e12-39155acd941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 1234\n",
    "\n",
    "models = {\n",
    "    \"DecisionTree\": {\n",
    "        \"model\": DecisionTreeClassifier(max_depth=4, random_state=random_state),\n",
    "    },\n",
    "    \"LinearSvc\": {\n",
    "        \"model\": LinearSVC(max_iter=10_000, random_state=random_state),\n",
    "    },\n",
    "    \"LogisticRegression\": {\n",
    "        \"model\": LogisticRegression(),\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"model\": RandomForestClassifier(max_depth=4, random_state=random_state),\n",
    "        #\"param_grid\": {\"model__n_estimators\": [5, 10], \"model__max_depth\": [None, 5, 10]},\n",
    "    },\n",
    "    \"SvcWithRbfKernel\": {\n",
    "        \"model\": SVC(kernel=\"rbf\", gamma=0.7, random_state=0),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6c2680-f220-4284-8252-d6e7a64bf39a",
   "metadata": {},
   "source": [
    "### Set metrics parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da53fb9c-7f6b-4ea7-b09c-032ce5b34be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = \"all\"  # for confusion matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23a9c96-ac8b-4164-a81b-bc46fe55ca09",
   "metadata": {},
   "source": [
    "###  1 -  `LogisticRegression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d8f2417-d62b-42d7-971b-0242500ef0cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A given column is not a column of the dataframe",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\project_env\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3653\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3654\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\project_env\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\project_env\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Location'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\project_env\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m_get_column_indices\u001b[1;34m(X, key)\u001b[0m\n\u001b[0;32m    446\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 447\u001b[1;33m                 \u001b[0mcol_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_columns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    448\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\project_env\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3654\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3655\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3656\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Location'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17912\\1454143090.py\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m### Train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictors_feature_engineering_transformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# apply fit and transform, successively\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# Evaluate and display metrics on `train` and `val` splits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\project_env\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1150\u001b[0m                 )\n\u001b[0;32m   1151\u001b[0m             ):\n\u001b[1;32m-> 1152\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\project_env\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[0mfit_params_last_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fit_transform\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 479\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mlast_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    480\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mlast_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\project_env\\lib\\site-packages\\sklearn\\utils\\_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m         \u001b[0mdata_to_wrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[1;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Weather_Project\\weather-prediction\\src\\weather\\transformers\\skl_transformer_utilities.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;31m# Fit and transform the data and convert to DataFrame in one step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[0mx_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;31m# Get the output feature names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\project_env\\lib\\site-packages\\sklearn\\utils\\_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m         \u001b[0mdata_to_wrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[1;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\project_env\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1150\u001b[0m                 )\n\u001b[0;32m   1151\u001b[0m             ):\n\u001b[1;32m-> 1152\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\project_env\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    749\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_transformers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_column_callables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    752\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_remainder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\project_env\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\u001b[0m in \u001b[0;36m_validate_column_callables\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    457\u001b[0m                 \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m             \u001b[0mall_columns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 459\u001b[1;33m             \u001b[0mtransformer_to_input_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_column_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\project_env\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m_get_column_indices\u001b[1;34m(X, key)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 455\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"A given column is not a column of the dataframe\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcolumn_indices\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: A given column is not a column of the dataframe"
     ]
    }
   ],
   "source": [
    "model_name = \"LogisticRegression\"\n",
    "model = models[model_name][\"model\"]\n",
    "score = f1_score\n",
    "model_subdir  = models_dir / camel_to_snake(model_name)\n",
    "model_subdir.mkdir(exist_ok=True)\n",
    "\n",
    "# Persist first three pipelines\n",
    "joblib.dump(dataset_ingestion_transformer, model_subdir / \"dataset_ingestion_pipeline.pkl\")\n",
    "joblib.dump(remove_horizonless_rows_transformer, model_subdir / \"remove_horizonless_rows_pipeline.pkl\")\n",
    "joblib.dump(target_creation_transformer, model_subdir / \"target_creation_pipeline.pkl\")\n",
    "\n",
    "### Train\n",
    "model.fit(predictors_feature_engineering_transformer.fit_transform(dataset.train_x), dataset.train_y) # apply fit and transform, successively\n",
    "\n",
    "# Evaluate and display metrics on `train` and `val` splits\n",
    "print(train_val_score_evaluation(score, predictors_feature_engineering_transformer, model, dataset))\n",
    "\n",
    "# Persit \"train_val_...\" pipelines\n",
    "joblib.dump(predictors_feature_engineering_transformer, model_subdir / \"train_val_predictors_feature_eng_pipeline.pkl\")\n",
    "joblib.dump(model, model_subdir / \"train_val_model.pkl\");\n",
    "\n",
    "# Concatenate the `train` and `val` splits\n",
    "dataset.concatenate_train_and_val_splits()\n",
    "\n",
    "### Retrain \n",
    "model.fit(predictors_feature_engineering_transformer.fit_transform(dataset.train_val_x), dataset.train_val_y)\n",
    "\n",
    "# Evaluate and display metrics on `train_val` and `test` splits\n",
    "print(train_test_score_evaluation(score, predictors_feature_engineering_transformer, model, dataset))\n",
    "\n",
    "# Perist \"train_test_...\"  pipelines\n",
    "joblib.dump(predictors_feature_engineering_transformer, model_subdir / \"train_test_predictors_feature_eng_pipeline.pkl\")\n",
    "joblib.dump(model, model_subdir / \"train_test_model.pkl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e72524-6773-455d-b9c0-5a907950e0a3",
   "metadata": {},
   "source": [
    "### 2 - `LinearSvc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bba4a65-4226-47cb-83ac-8edf4073dec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainValScore(score_name='f1_score', train=0.935, val=0.933)\n",
      "TrainTestScore(score_name='f1_score', train_val=0.933, test=0.916)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"LinearSvc\"\n",
    "model = models[model_name][\"model\"]\n",
    "score = f1_score\n",
    "model_subdir  = models_dir / camel_to_snake(model_name)\n",
    "model_subdir.mkdir(exist_ok=True)\n",
    "\n",
    "# Persist first three pipelines\n",
    "joblib.dump(dataset_ingestion_transformer, model_subdir / \"dataset_ingestion_pipeline.pkl\")\n",
    "joblib.dump(remove_horizonless_rows_transformer, model_subdir / \"remove_horizonless_rows_pipeline.pkl\")\n",
    "joblib.dump(target_creation_transformer, model_subdir / \"target_creation_pipeline.pkl\")\n",
    "\n",
    "### Train\n",
    "model.fit(predictors_feature_engineering_transformer.fit_transform(dataset.train_x), dataset.train_y) # apply fit and transform, successively\n",
    "\n",
    "# Evaluate and display metrics on `train` and `val` splits\n",
    "print(train_val_score_evaluation(score, predictors_feature_engineering_transformer, model, dataset))\n",
    "\n",
    "# Persit \"train_val_...\" pipelines\n",
    "joblib.dump(predictors_feature_engineering_transformer, model_subdir / \"train_val_predictors_feature_eng_pipeline.pkl\")\n",
    "joblib.dump(model, model_subdir / \"train_val_model.pkl\");\n",
    "\n",
    "# Concatenate the `train` and `val` splits\n",
    "dataset.concatenate_train_and_val_splits()\n",
    "\n",
    "### Retrain \n",
    "model.fit(predictors_feature_engineering_transformer.fit_transform(dataset.train_val_x), dataset.train_val_y)\n",
    "\n",
    "# Evaluate and display metrics on `train_val` and `test` splits\n",
    "print(train_test_score_evaluation(score, predictors_feature_engineering_transformer, model, dataset))\n",
    "\n",
    "# Perist \"train_test_...\"  pipelines\n",
    "joblib.dump(predictors_feature_engineering_transformer, model_subdir / \"train_test_predictors_feature_eng_pipeline.pkl\")\n",
    "joblib.dump(model, model_subdir / \"train_test_model.pkl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f03125-74b6-4ce9-bb1c-c47049827932",
   "metadata": {},
   "source": [
    "### 3 - `SvcWithRbfKernel` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f339563-458c-4632-a152-be2c851bf998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"SvcWithRbfKernel\"\n",
    "# model = models[model_name][\"model\"]\n",
    "# score = f1_score\n",
    "# model_subdir  = models_dir / camel_to_snake(model_name)\n",
    "# model_subdir.mkdir(exist_ok=True)\n",
    "\n",
    "# # Persist first three pipelines\n",
    "# joblib.dump(dataset_ingestion_transformer, model_subdir / \"dataset_ingestion_pipeline.pkl\")\n",
    "# joblib.dump(remove_horizonless_rows_transformer, model_subdir / \"remove_horizonless_rows_pipeline.pkl\")\n",
    "# joblib.dump(target_creation_transformer, model_subdir / \"target_creation_pipeline.pkl\")\n",
    "\n",
    "# ### Train\n",
    "# model.fit(predictors_feature_engineering_transformer.fit_transform(dataset.train_x), dataset.train_y) # apply fit and transform, successively\n",
    "\n",
    "# # Evaluate and display metrics on `train` and `val` splits\n",
    "# print(train_val_score_evaluation(score, predictors_feature_engineering_transformer, model, dataset))\n",
    "\n",
    "# # Persit \"train_val_...\" pipelines\n",
    "# joblib.dump(predictors_feature_engineering_transformer, model_subdir / \"train_val_predictors_feature_eng_pipeline.pkl\")\n",
    "# joblib.dump(model, model_subdir / \"train_val_model.pkl\");\n",
    "\n",
    "# # Concatenate the `train` and `val` splits\n",
    "# dataset.concatenate_train_and_val_splits()\n",
    "\n",
    "# ### Retrain \n",
    "# model.fit(predictors_feature_engineering_transformer.fit_transform(dataset.train_val_x), dataset.train_val_y)\n",
    "\n",
    "# # Evaluate and display metrics on `train_val` and `test` splits\n",
    "# print(train_test_score_evaluation(score, predictors_feature_engineering_transformer, model, dataset))\n",
    "\n",
    "# # Perist \"train_test_...\"  pipelines\n",
    "# joblib.dump(predictors_feature_engineering_transformer, model_subdir / \"train_test_predictors_feature_eng_pipeline.pkl\")\n",
    "# joblib.dump(model, model_subdir / \"train_test_model.pkl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cb1229-ee03-4575-91fa-d016ddaa795c",
   "metadata": {},
   "source": [
    "### 4 - `DecisionTree`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f9c9524-b98a-4eec-806f-420b31039710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainValScore(score_name='f1_score', train=0.951, val=0.933)\n",
      "TrainTestScore(score_name='f1_score', train_val=0.952, test=0.948)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"DecisionTree\"\n",
    "model = models[model_name][\"model\"]\n",
    "score = f1_score\n",
    "model_subdir  = models_dir / camel_to_snake(model_name)\n",
    "model_subdir.mkdir(exist_ok=True)\n",
    "\n",
    "# Persist first three pipelines\n",
    "joblib.dump(dataset_ingestion_transformer, model_subdir / \"dataset_ingestion_pipeline.pkl\")\n",
    "joblib.dump(remove_horizonless_rows_transformer, model_subdir / \"remove_horizonless_rows_pipeline.pkl\")\n",
    "joblib.dump(target_creation_transformer, model_subdir / \"target_creation_pipeline.pkl\")\n",
    "\n",
    "### Train\n",
    "model.fit(predictors_feature_engineering_transformer.fit_transform(dataset.train_x), dataset.train_y) # apply fit and transform, successively\n",
    "\n",
    "# Evaluate and display metrics on `train` and `val` splits\n",
    "print(train_val_score_evaluation(score, predictors_feature_engineering_transformer, model, dataset))\n",
    "\n",
    "# Persit \"train_val_...\" pipelines\n",
    "joblib.dump(predictors_feature_engineering_transformer, model_subdir / \"train_val_predictors_feature_eng_pipeline.pkl\")\n",
    "joblib.dump(model, model_subdir / \"train_val_model.pkl\");\n",
    "\n",
    "# Concatenate the `train` and `val` splits\n",
    "dataset.concatenate_train_and_val_splits()\n",
    "\n",
    "### Retrain \n",
    "model.fit(predictors_feature_engineering_transformer.fit_transform(dataset.train_val_x), dataset.train_val_y)\n",
    "\n",
    "# Evaluate and display metrics on `train_val` and `test` splits\n",
    "print(train_test_score_evaluation(score, predictors_feature_engineering_transformer, model, dataset))\n",
    "\n",
    "# Perist \"train_test_...\"  pipelines\n",
    "joblib.dump(predictors_feature_engineering_transformer, model_subdir / \"train_test_predictors_feature_eng_pipeline.pkl\")\n",
    "joblib.dump(model, model_subdir / \"train_test_model.pkl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a70d77-c4fd-4045-b9fd-8e36c2b6905c",
   "metadata": {},
   "source": [
    "### 5 - `RandomForest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fefbfca8-5369-413e-a96a-321b5ea5b202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainValScore(score_name='f1_score', train=0.952, val=0.942)\n",
      "TrainTestScore(score_name='f1_score', train_val=0.952, test=0.952)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"RandomForest\"\n",
    "model = models[model_name][\"model\"]\n",
    "score = f1_score\n",
    "model_subdir  = models_dir / camel_to_snake(model_name)\n",
    "model_subdir.mkdir(exist_ok=True)\n",
    "\n",
    "# Persist first three pipelines\n",
    "joblib.dump(dataset_ingestion_transformer, model_subdir / \"dataset_ingestion_pipeline.pkl\")\n",
    "joblib.dump(remove_horizonless_rows_transformer, model_subdir / \"remove_horizonless_rows_pipeline.pkl\")\n",
    "joblib.dump(target_creation_transformer, model_subdir / \"target_creation_pipeline.pkl\")\n",
    "\n",
    "### Train\n",
    "model.fit(predictors_feature_engineering_transformer.fit_transform(dataset.train_x), dataset.train_y) # apply fit and transform, successively\n",
    "\n",
    "# Evaluate and display metrics on `train` and `val` splits\n",
    "print(train_val_score_evaluation(score, predictors_feature_engineering_transformer, model, dataset))\n",
    "\n",
    "# Persit \"train_val_...\" pipelines\n",
    "joblib.dump(predictors_feature_engineering_transformer, model_subdir / \"train_val_predictors_feature_eng_pipeline.pkl\")\n",
    "joblib.dump(model, model_subdir / \"train_val_model.pkl\");\n",
    "\n",
    "# Concatenate the `train` and `val` splits\n",
    "dataset.concatenate_train_and_val_splits()\n",
    "\n",
    "### Retrain \n",
    "model.fit(predictors_feature_engineering_transformer.fit_transform(dataset.train_val_x), dataset.train_val_y)\n",
    "\n",
    "# Evaluate and display metrics on `train_val` and `test` splits\n",
    "print(train_test_score_evaluation(score, predictors_feature_engineering_transformer, model, dataset))\n",
    "\n",
    "# Perist \"train_test_...\"  pipelines\n",
    "joblib.dump(predictors_feature_engineering_transformer, model_subdir / \"train_test_predictors_feature_eng_pipeline.pkl\")\n",
    "joblib.dump(model, model_subdir / \"train_test_model.pkl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0054456-6ce2-4d74-96d1-1b8d3c42bd70",
   "metadata": {},
   "source": [
    "### 6. Test metrics consistancy with pikled pipelines and  models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88072f74-09f9-4b7e-95ea-0b58fd92a874",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_transformer = joblib.load(model_subdir / \"dataset_ingestion_pipeline.pkl\")\n",
    "remove_horizonless_rows_transformer = joblib.load(model_subdir / \"remove_horizonless_rows_pipeline.pkl\")\n",
    "target_creation_transformer = joblib.load(model_subdir / \"target_creation_pipeline.pkl\")\n",
    "train_val_predictors_feature_engineering_transformer =joblib.load(model_subdir / \"train_val_predictors_feature_eng_pipeline.pkl\")\n",
    "train_test_predictors_feature_engineering_transformer =joblib.load(model_subdir / \"train_test_predictors_feature_eng_pipeline.pkl\")\n",
    "train_val_model = joblib.load(model_subdir / \"train_val_model.pkl\")\n",
    "train_test_model = joblib.load(model_subdir / \"train_test_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972aa2ed-dd36-498f-822d-b9ba14b3092f",
   "metadata": {},
   "source": [
    "### Transform the dataset and split it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cd37c0f-2bc3-47e1-9353-ffc2825bb0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform\n",
    "transformed_data, created_target = transform_dataset_and_create_target(\n",
    "    df,   \n",
    "    dataset_ingestion_transformer,\n",
    "    remove_horizonless_rows_transformer,\n",
    "    target_creation_transformer,\n",
    ")\n",
    "\n",
    "# Split\n",
    "dataset = prepare_binary_classification_tabular_data(\n",
    "    transformed_data,\n",
    "    created_target,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f769fc-8966-4e3f-8427-8eaeb10b43ba",
   "metadata": {},
   "source": [
    "### Display `f1-score`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bba2a32e-d6a4-4244-88a0-d17e9bc9079c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainValScore(score_name='f1_score', train=0.952, val=0.942)\n",
      "TrainTestScore(score_name='f1_score', train_val=0.952, test=0.952)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate and display metrics on `train` and `val` splits\n",
    "print(train_val_score_evaluation(score, train_val_predictors_feature_engineering_transformer, train_val_model, dataset))\n",
    "\n",
    "# Concatenate the `train` and `val` splits\n",
    "dataset.concatenate_train_and_val_splits()\n",
    "\n",
    "# Evaluate and display metrics on `train_val` and `test` splits\n",
    "print(train_test_score_evaluation(score, train_test_predictors_feature_engineering_transformer, train_test_model, dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba62da8-0874-409f-bf38-d8128b02ce49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
